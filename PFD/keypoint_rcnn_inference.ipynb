{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/PFD\n"
     ]
    }
   ],
   "source": [
    "%cd PFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2, numpy as np, matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.models.detection import KeypointRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import torchvision.transforms as T\n",
    "import onnx\n",
    "import onnxruntime as onnxrt\n",
    "from onnx_tf.backend import prepare\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(weights=None,\n",
    "                                                                   weights_backbone=ResNet50_Weights.DEFAULT,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator, \n",
    "                                                                   min_size=512)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = get_model(num_keypoints = 4, weights_path='./keypoint_model/weights/keypointsrcnn_weights.pth')\n",
    "model.to(device)\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Frame filter Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bbox_center(bbox):\n",
    "    # calculate center of bbox\n",
    "    bbox_width = abs(bbox[0] - bbox[2])\n",
    "    bbox_height = abs(bbox[1] - bbox[3])\n",
    "\n",
    "    return [bbox[0] + bbox_width / 2, bbox[1] + bbox_height / 2], bbox_width, bbox_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_frame_filter(keypoint, bbox):\n",
    "    filtered_keypoints = []\n",
    "    filtered_frames = []\n",
    "    # filter threshold\n",
    "    x_threshold = 0.5\n",
    "    y_threshold = 6\n",
    "\n",
    "    # calculate center of bbox\n",
    "    bbox_center, bbox_width, bbox_height = calculate_bbox_center(bbox)\n",
    "\n",
    "    # find left and right side coordinates\n",
    "    left_side_coordinates = []\n",
    "    right_side_coordinates = []\n",
    "\n",
    "    # Get the left and right side coordinates\n",
    "    for point in keypoint:\n",
    "        if bbox_center[0] < point[0]:\n",
    "            left_side_coordinates.append(point)\n",
    "        elif bbox_center[0] > point[0]:\n",
    "            right_side_coordinates.append(point)\n",
    "\n",
    "    if len(left_side_coordinates) != 2 or len(right_side_coordinates) != 2:\n",
    "        return False\n",
    "\n",
    "    # Get the top and bottom side coordinates\n",
    "    top_side_coordinates = []\n",
    "    bottom_side_coordinates = []\n",
    "\n",
    "    for point in keypoint:\n",
    "        if bbox_center[1] < point[1]:\n",
    "            top_side_coordinates.append(point)\n",
    "        elif bbox_center[1] > point[1]:\n",
    "            bottom_side_coordinates.append(point)\n",
    "    \n",
    "    if len(top_side_coordinates) != 2 or len(bottom_side_coordinates) != 2:\n",
    "        return False\n",
    "\n",
    "    # Extract the slope angles\n",
    "    left_side_slope = 0 if left_side_coordinates[0][0] - left_side_coordinates[1][0] == 0 else (left_side_coordinates[0][1] - left_side_coordinates[1][1]) / (left_side_coordinates[0][0] - left_side_coordinates[1][0])\n",
    "    right_side_slope = 0 if right_side_coordinates[0][0] - right_side_coordinates[1][0] == 0 else (right_side_coordinates[0][1] - right_side_coordinates[1][1]) / (right_side_coordinates[0][0] - right_side_coordinates[1][0])\n",
    "    top_side_slope = 0 if top_side_coordinates[1][0] - top_side_coordinates[0][0] == 0 else (top_side_coordinates[1][1] - top_side_coordinates[0][1]) / (top_side_coordinates[1][0] - top_side_coordinates[0][0])\n",
    "    bottom_side_slope = 0 if bottom_side_coordinates[1][0] - bottom_side_coordinates[0][0] == 0 else (bottom_side_coordinates[1][1] - bottom_side_coordinates[0][1]) / (bottom_side_coordinates[1][0] - bottom_side_coordinates[0][0])\n",
    "\n",
    "    diff_top_bottom = abs(top_side_slope - bottom_side_slope)\n",
    "    diff_left_right = abs(left_side_slope - right_side_slope)\n",
    "\n",
    "    if diff_top_bottom < x_threshold and diff_left_right < y_threshold:\n",
    "        return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "bbox_buffer = 5\n",
    "\n",
    "def is_point_inside_rect(point, rect):\n",
    "    x, y = point\n",
    "    top_left_x, top_left_y, bottom_right_x, bottom_right_y = rect\n",
    "    rect_x = top_left_x - bbox_buffer\n",
    "    rect_y = top_left_y - bbox_buffer\n",
    "    rect_w = bottom_right_x - top_left_x + bbox_buffer\n",
    "    rect_h = bottom_right_y - top_left_y + bbox_buffer\n",
    "    return rect_x <= x <= rect_x + rect_w and rect_y <= y <= rect_y + rect_h\n",
    "\n",
    "def isHandInFrame(input_frame, bbox):\n",
    "    # Initialize fresh pose tracker and run it.\n",
    "      with mp_pose.Pose() as pose_tracker:\n",
    "        result = pose_tracker.process(image=input_frame)\n",
    "        pose_landmarks = result.pose_landmarks\n",
    "        \n",
    "        # Save landmarks.\n",
    "        if pose_landmarks is not None:\n",
    "            # Check the number of landmarks and take pose landmarks.\n",
    "            assert len(pose_landmarks.landmark) == 33, 'Unexpected number of predicted pose landmarks: {}'.format(len(pose_landmarks.landmark))\n",
    "\n",
    "            pose_landmarks = [[lmk.x, lmk.y, lmk.z] for lmk in pose_landmarks.landmark]\n",
    "\n",
    "            # only extract upper body\n",
    "            pose_landmarks = pose_landmarks[:25]\n",
    "\n",
    "            # Map pose landmarks from [0, 1] range to absolute coordinates to get\n",
    "            # correct aspect ratio.\n",
    "            frame_height, frame_width = input_frame.shape[:2]\n",
    "            pose_landmarks *= np.array([frame_width, frame_height, frame_width])\n",
    "\n",
    "            # check if pose is inside keypoint\n",
    "            for pose in pose_landmarks:\n",
    "                if not is_point_inside_rect(pose[:2], bbox):\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on Live Camera or Video Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame capture rate in seconds\n",
    "capture_rate = 10\n",
    "\n",
    "# video file\n",
    "video_file = \"./assets/pfd_video_dataset/demo_0227.mp4\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on video recording \n",
    "Choose from:\n",
    "* Display video and draw over bounding box\n",
    "* Extract frames for further processing (saves more time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with displaying video\n",
    "frame_list = []\n",
    "keypoint_list = []\n",
    "bbox_list = []\n",
    "\n",
    "# Open the video file\n",
    "video = cv2.VideoCapture(video_file)\n",
    "\n",
    "# Check if video is opened successfully\n",
    "if not video.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "\n",
    "# Get the width and height of the video\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Set the desired width\n",
    "desired_width = 500\n",
    "\n",
    "# Calculate the aspect ratio\n",
    "aspect_ratio = height / width\n",
    "\n",
    "# Calculate the new height\n",
    "desired_height = int(desired_width * aspect_ratio)\n",
    "\n",
    "# Get the frames per second of the video\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Calculate the number of frames to skip\n",
    "skip_frames = int(fps * capture_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display video and draw keypoints and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the counter for frames\n",
    "frame_count = 0\n",
    "\n",
    "# keypoint, bbox and frame lists \n",
    "keypoints_main = []\n",
    "bboxes_main = []\n",
    "\n",
    "# Create a VideoWriter object to write the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # codec\n",
    "output_video = cv2.VideoWriter('output_video.mp4', fourcc, fps, (desired_width, desired_height))\n",
    "\n",
    "# Read until video is completed\n",
    "while(video.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    if frame_count % skip_frames == 0:\n",
    "        bboxes_main = []\n",
    "        keypoints_main = []\n",
    "\n",
    "        frame_np = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = F.to_tensor(frame_np)\n",
    "        frame_tensor = frame_tensor.to(device)\n",
    "        \n",
    "        images = [frame_tensor]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            output = model(images)\n",
    "        \n",
    "        frame_tensor = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        \n",
    "        high_scores_idxs = np.where(scores > 0.8)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "        \n",
    "        keypoints = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            \n",
    "        bboxes = []\n",
    "        for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            bboxes.append(list(map(int, bbox.tolist())))\n",
    "\n",
    "\n",
    "        #filter keypoints\n",
    "        if len(keypoints) != 0 and live_frame_filter(keypoints[0], bboxes[0]) and isHandInFrame(frame_np, bboxes[0]) == False:\n",
    "            if(len(keypoints) != 0):\n",
    "                keypoints_main = keypoints\n",
    "            \n",
    "            if(len(bboxes) != 0):\n",
    "                bboxes_main = bboxes\n",
    "\n",
    "        # add frame to to frame_list\n",
    "        frame_list.append(cv2.cvtColor(frame.copy(), cv2.COLOR_BGR2RGB))\n",
    "        keypoint_list.append(keypoints_main)\n",
    "        bbox_list.append(bboxes_main)\n",
    "\n",
    "    if(len(keypoints_main) != 0):\n",
    "        points = np.array(keypoints_main[0], np.int32)\n",
    "        points = points.reshape((-1, 1, 2))\n",
    "\n",
    "        # Draw a keypoints on the frame\n",
    "        # cv2.polylines(frame, [points], isClosed=True, color=(0, 0, 0), thickness=8)\n",
    "        for point in points:\n",
    "            center = (point[0][0], point[0][1])\n",
    "            radius = 15\n",
    "\n",
    "            # Draw the keypoints using the circle() function\n",
    "            cv2.circle(frame, center, radius, (0, 0, 255), -1)\n",
    "\n",
    "        if(len(bboxes_main) != 0):\n",
    "            # draw a bbox on the frame\n",
    "            cv2.rectangle(frame, (bboxes_main[0][0], bboxes_main[0][1]),  (bboxes_main[0][2], bboxes_main[0][3]), color=(0, 255, 0), thickness=8)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    frame = cv2.resize(frame, (desired_width, desired_height), interpolation=cv2.INTER_AREA)\n",
    "    # cv2.imshow('Frame', frame)\n",
    "    # Write the modified frame to the output video\n",
    "    output_video.write(frame)\n",
    "    \n",
    "    # Press Q on keyboard to exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_count = frame_count + 1\n",
    "# Release the video\n",
    "video.release()\n",
    "output_video.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract frames and keypoints for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of frames in the video\n",
    "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# keypoint, bbox and frame lists \n",
    "keypoints_main = []\n",
    "bboxes_main = []\n",
    "\n",
    "# Loop through the frames of the video\n",
    "for i in range(total_frames):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if i % skip_frames == 0:\n",
    "        bboxes_main = []\n",
    "        keypoints_main = []\n",
    "\n",
    "        frame_np = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)            \n",
    "        frame_tensor = F.to_tensor(frame_np)\n",
    "        frame_tensor = frame_tensor.to(device)\n",
    "\n",
    "        images = [frame_tensor]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            output = model(images)\n",
    "\n",
    "        frame_tensor = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "        high_scores_idxs = np.where(scores > 0.8)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        keypoints = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            \n",
    "        bboxes = []\n",
    "        for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            bboxes.append(list(map(int, bbox.tolist())))\n",
    "\n",
    "        #filter keypoints\n",
    "        if len(keypoints) != 0 and live_frame_filter(keypoints[0], bboxes[0]) and isHandInFrame(frame_np, bboxes[0]) == False:\n",
    "            if(len(keypoints) != 0):\n",
    "                keypoints_main = keypoints\n",
    "            \n",
    "            if(len(bboxes) != 0):\n",
    "                bboxes_main = bboxes\n",
    "        \n",
    "        # add frame to to frame_list\n",
    "        if len(keypoints_main) > 0:\n",
    "            frame_list.append(frame_tensor)\n",
    "            keypoint_list.append([keypoints_main[0]])\n",
    "            bbox_list.append(bboxes)\n",
    "\n",
    "# Release the video\n",
    "video.release()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on live camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the counter for frames\n",
    "frame_count = 0\n",
    "\n",
    "# keypoint, bbox and frame lists \n",
    "keypoints_main = []\n",
    "bboxes_main = []\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(0) # 0 means the default camera\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if int(time.time()) % capture_rate == 0:\n",
    "        frame_np = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = F.to_tensor(frame_np)\n",
    "        frame_tensor = frame_tensor.to(device)\n",
    "        images = [frame_tensor]\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            output = model(images)\n",
    "        \n",
    "        frame_tensor = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        \n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        keypoints = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            \n",
    "        bboxes = []\n",
    "        for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "        if(len(keypoints) != 0):\n",
    "            print(keypoints)\n",
    "            keypoints_main = keypoints\n",
    "        \n",
    "        if(len(bboxes) != 0):\n",
    "            print(bboxes)\n",
    "            bboxes_main = bboxes\n",
    "\n",
    "        # add frame to to frame_list\n",
    "        frame_list.append(frame)\n",
    "        keypoint_list.append(keypoints_main)\n",
    "        bbox_list.append(bboxes_main)\n",
    "\n",
    "    if(len(keypoints_main) != 0):\n",
    "        points = np.array(keypoints_main, np.int32)\n",
    "        points = points.reshape((-1, 1, 2))\n",
    "\n",
    "        # Draw a keypoints on the frame\n",
    "        # cv2.polylines(frame, [points], isClosed=True, color=(0, 0, 0), thickness=8)\n",
    "        for point in points:\n",
    "            center = (point[0][0], point[0][1])\n",
    "            radius = 15\n",
    "\n",
    "            # Draw the keypoints using the circle() function\n",
    "            cv2.circle(frame, center, radius, (255, 0, 0), -1)\n",
    "\n",
    "    if(len(bboxes_main) != 0):\n",
    "        # draw a bbox on the frame\n",
    "        cv2.rectangle(frame, (bboxes_main[0][0], bboxes_main[0][1]),  (bboxes_main[0][2], bboxes_main[0][3]), color=(0, 255, 0), thickness=8)\n",
    "        \n",
    "    # Display the frame with the bounding box\n",
    "    cv2.imshow(\"Live Video with Bounding Box\", frame)\n",
    "\n",
    "    # `Exit the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_count = frame_count + 1\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transformation and Timelapse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perspective Transformation and Timelapse Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frame rate\n",
    "num_frames = len(frame_list)\n",
    "timelapse_length = num_frames / 2\n",
    "frame_rate = int(num_frames / timelapse_length)\n",
    "\n",
    "#\n",
    "\n",
    "# previous frame for blending\n",
    "prev_frame = []\n",
    "# keypoint = keypoint_list[0][0]\n",
    "# bbox = bbox_list[0][0]\n",
    "\n",
    "bbox_center, bbox_width, bbox_height = calculate_bbox_center(bbox)\n",
    "\n",
    "#  Define the codec and create a video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MP4V\" )\n",
    "out = cv2.VideoWriter(\"timelapse.mp4\", fourcc, frame_rate, (bbox_width, bbox_height))\n",
    "\n",
    "\n",
    "for count, frame in enumerate(frame_list):\n",
    "    keypoint = keypoint_list[count][0]\n",
    "    bbox = bbox_list[count][0]\n",
    "\n",
    "    # pt1 => (top_left, top_right, bottom_left, bottom_right)\n",
    "    top_left = []\n",
    "    top_right = []\n",
    "    bottom_left = []\n",
    "    bottom_right = []\n",
    "\n",
    "    for kp in keypoint:\n",
    "        print(kp)\n",
    "        if kp[0] < bbox_center[0] and kp[1] < bbox_center[1]:\n",
    "            top_left = kp\n",
    "        elif kp[0] > bbox_center[0] and kp[1] > bbox_center[1]:\n",
    "            bottom_right = kp\n",
    "        elif kp[0] < bbox_center[0] and kp[1] > bbox_center[1]:\n",
    "            top_right = kp\n",
    "        elif kp[0] > bbox_center[0] and kp[1] < bbox_center[1]:\n",
    "            bottom_left = kp\n",
    "\n",
    "    pt1 = np.float32([top_left, bottom_left, top_right, bottom_right])\n",
    "    pt2 = np.float32([[0,0],[bbox_width, 0], [0, bbox_height], [bbox_width, bbox_height]])\n",
    "    M = cv2.getPerspectiveTransform(pt1,pt2)\n",
    "\n",
    "    # Perform the transformation\n",
    "    warped_image = cv2.warpPerspective(frame, M, (bbox_width, bbox_height))\n",
    "    warped_image = cv2.cvtColor(warped_image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # blend frames together for better transition\n",
    "    if count != 0:\n",
    "        blended = cv2.addWeighted(prev_frame, 0.1, warped_image, 0.9, 0)\n",
    "    else:\n",
    "        blended = warped_image\n",
    "    prev_frame = warped_image\n",
    "\n",
    "    out.write(blended)\n",
    "\n",
    "# Release the video writer\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d32610e65d0ba547d20b5ddccd11b31c7d91644470808fb362c82b58c120951"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
