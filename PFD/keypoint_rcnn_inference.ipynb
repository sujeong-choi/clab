{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        \n",
    "        # update state_dict keys\n",
    "        # for key in list(state_dict.keys()):\n",
    "        #     state_dict[key.replace('.0.0', '.0').replace('.1.0', '.1').replace('.2.0', '.2').replace('.3.0', '.3').replace('rpn.head.conv.0.0.', 'rpn.head.conv.').replace('rpn.head.conv.0.', 'rpn.head.conv.')] = state_dict.pop(key)\n",
    "\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hiski\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hiski\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\hiski\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and may be removed in the future, please use 'weights_backbone' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hiski\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights_backbone=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights_backbone=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KeypointRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(640, 672, 704, 736, 768, 800), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (keypoint_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (keypoint_head): KeypointRCNNHeads(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "    )\n",
       "    (keypoint_predictor): KeypointRCNNPredictor(\n",
       "      (kps_score_lowres): ConvTranspose2d(512, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = get_model(num_keypoints = 4, weights_path='./assets/keypoint_model/weights/keypointsrcnn_weights 001.pth')\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on Live Camera or Video Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame capture rate in seconds\n",
    "capture_rate = 5\n",
    "\n",
    "# keypoint, bbox and frame lists \n",
    "keypoints_main = []\n",
    "bboxes_main = []\n",
    "frame_list = []\n",
    "keypoint_list = []\n",
    "bbox_list = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the video file\n",
    "video = cv2.VideoCapture(\"./assets/pfd_video_dataset/20221014_124610.mp4\")\n",
    "\n",
    "# Check if video is opened successfully\n",
    "if not video.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "\n",
    "# Read until video is completed\n",
    "while(video.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video.read()\n",
    "    \n",
    "    if ret == True:\n",
    "        if int(time.time()) % capture_rate == 0:\n",
    "            frame_np = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)            \n",
    "            frame_tensor = F.to_tensor(frame_np)\n",
    "            frame_tensor = frame_tensor.to(device)\n",
    "            \n",
    "            images = [frame_tensor]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.to(device)\n",
    "                model.eval()\n",
    "                output = model(images)\n",
    "            \n",
    "            frame_tensor = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "            scores = output[0]['scores'].detach().cpu().numpy()\n",
    "            \n",
    "            high_scores_idxs = np.where(scores > 0.8)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "            post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "            keypoints = []\n",
    "            for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "                keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "                \n",
    "            bboxes = []\n",
    "            for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "\n",
    "            if(len(keypoints) != 0):\n",
    "                keypoints_main = keypoints\n",
    "            \n",
    "            if(len(bboxes) != 0):\n",
    "                bboxes_main = bboxes\n",
    "\n",
    "            # add frame to to frame_list\n",
    "            frame_list.append(frame)\n",
    "            keypoint_list.append(keypoints_main)\n",
    "            bbox_list.append(bboxes_main)\n",
    "\n",
    "        if(len(keypoints_main) != 0):\n",
    "            points = np.array(keypoints_main, np.int32)\n",
    "            points = points.reshape((-1, 1, 2))\n",
    "\n",
    "            # Draw a keypoints on the frame\n",
    "            # cv2.polylines(frame, [points], isClosed=True, color=(0, 0, 0), thickness=8)\n",
    "            for point in points:\n",
    "                center = (point[0][0], point[0][1])\n",
    "                radius = 15\n",
    "\n",
    "                # Draw the keypoints using the circle() function\n",
    "                cv2.circle(frame, center, radius, (0, 0, 255), -1)\n",
    "\n",
    "        if(len(bboxes_main) != 0):\n",
    "            # draw a bbox on the frame\n",
    "            cv2.rectangle(frame, (bboxes_main[0][0], bboxes_main[0][1]),  (bboxes_main[0][2], bboxes_main[0][3]), color=(0, 255, 0), thickness=8)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        frame = cv2.resize(frame, (600, 800), interpolation=cv2.INTER_AREA)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        \n",
    "        # Press Q on keyboard to exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Break the loop\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video\n",
    "video.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on live camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(0) # 0 means the default camera\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if int(time.time()) % capture_rate == 0:\n",
    "        frame_np = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = F.to_tensor(frame_np)\n",
    "        frame_tensor = frame_tensor.to(device)\n",
    "        images = [frame_tensor]\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            output = model(images)\n",
    "        \n",
    "        frame_tensor = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        \n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        keypoints = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            \n",
    "        bboxes = []\n",
    "        for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "        if(len(keypoints) != 0):\n",
    "            print(keypoints)\n",
    "            keypoints_main = keypoints\n",
    "        \n",
    "        if(len(bboxes) != 0):\n",
    "            print(bboxes)\n",
    "            bboxes_main = bboxes\n",
    "\n",
    "        # add frame to to frame_list\n",
    "        frame_list.append(frame)\n",
    "        keypoint_list.append(keypoints_main)\n",
    "        bbox_list.append(bboxes_main)\n",
    "\n",
    "    if(len(keypoints_main) != 0):\n",
    "        points = np.array(keypoints_main, np.int32)\n",
    "        points = points.reshape((-1, 1, 2))\n",
    "\n",
    "        # Draw a keypoints on the frame\n",
    "        # cv2.polylines(frame, [points], isClosed=True, color=(0, 0, 0), thickness=8)\n",
    "        for point in points:\n",
    "            center = (point[0][0], point[0][1])\n",
    "            radius = 15\n",
    "\n",
    "            # Draw the keypoints using the circle() function\n",
    "            cv2.circle(frame, center, radius, (255, 0, 0), -1)\n",
    "\n",
    "    if(len(bboxes_main) != 0):\n",
    "        # draw a bbox on the frame\n",
    "        cv2.rectangle(frame, (bboxes_main[0][0], bboxes_main[0][1]),  (bboxes_main[0][2], bboxes_main[0][3]), color=(0, 255, 0), thickness=8)\n",
    "        \n",
    "    # Display the frame with the bounding box\n",
    "    cv2.imshow(\"Live Video with Bounding Box\", frame)\n",
    "\n",
    "    # `Exit the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transformation and Timelapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of frames removed (threshold = 5): 65\n"
     ]
    }
   ],
   "source": [
    "filtered_keypoints = []\n",
    "filtered_frames = []\n",
    "\n",
    "# remove coordinates if its x or y is lower than the average + threshold\n",
    "for count, kp in enumerate(keypoint_list):\n",
    "    threshold = 5\n",
    "    coordinates = kp[0]\n",
    "\n",
    "    # Sort the coordinates based on x and y\n",
    "    coordinates = sorted(coordinates)\n",
    "\n",
    "    # Get the left and right side coordinates\n",
    "    left_side_coordinates = coordinates[:2]\n",
    "    right_side_coordinates = coordinates[2:]\n",
    "\n",
    "    # Get the top and bottom side coordinates\n",
    "    if left_side_coordinates[0][1] < left_side_coordinates[1][1]:\n",
    "        top_side_coordinates = left_side_coordinates\n",
    "        bottom_side_coordinates = right_side_coordinates\n",
    "    else:\n",
    "        top_side_coordinates = right_side_coordinates\n",
    "        bottom_side_coordinates = left_side_coordinates\n",
    "\n",
    "    # Extract the slope angles\n",
    "    left_side_slope = (top_side_coordinates[0][1] - top_side_coordinates[1][1]) / (top_side_coordinates[0][0] - top_side_coordinates[1][0])\n",
    "    right_side_slope = (bottom_side_coordinates[0][1] - bottom_side_coordinates[1][1]) / (bottom_side_coordinates[0][0] - bottom_side_coordinates[1][0])\n",
    "    top_side_slope = (top_side_coordinates[1][1] - top_side_coordinates[0][1]) / (top_side_coordinates[1][0] - top_side_coordinates[0][0])\n",
    "    bottom_side_slope = (bottom_side_coordinates[1][1] - bottom_side_coordinates[0][1]) / (bottom_side_coordinates[1][0] - bottom_side_coordinates[0][0])\n",
    "\n",
    "    diff_top_bottom = abs(top_side_slope - bottom_side_slope)\n",
    "    diff_left_right = abs(left_side_slope - right_side_slope)\n",
    "\n",
    "    if diff_top_bottom < threshold and diff_left_right < threshold:\n",
    "        filtered_keypoints.append(coordinates)\n",
    "        filtered_frames.append(frame_list[count])\n",
    "    \n",
    "removed = len(keypoint_list) - len(filtered_frames)\n",
    "print('Length of frames removed (threshold = {}): {}'.format(threshold, removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of frames removed (threshold = 10): 54\n"
     ]
    }
   ],
   "source": [
    "filtered_keypoints = []\n",
    "filtered_frames = []\n",
    "\n",
    "# get an average for each keypoint\n",
    "avg_keypoint = np.mean(keypoint_list, axis = 0)[0]\n",
    "\n",
    "# filter threshold\n",
    "threshold = 10\n",
    "\n",
    "# remove coordinates if its x or y is lower than the average + threshold\n",
    "for count, kp in enumerate(keypoint_list):\n",
    "    keypoint = kp[0]\n",
    "    \n",
    "    if (keypoint[0][0] < (avg_keypoint[0][0] + threshold) and keypoint[0][1] < (avg_keypoint[0][1] + threshold)) \\\n",
    "        and (keypoint[0][0] > (avg_keypoint[0][0] - threshold) and keypoint[0][1] > (avg_keypoint[0][1] - threshold)):\n",
    "        filtered_keypoints.append(keypoint)\n",
    "        filtered_frames.append(frame_list[count])\n",
    "\n",
    "removed = len(keypoint_list) - len(filtered_frames)\n",
    "print('Length of frames removed (threshold = {}): {}'.format(threshold, removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_list is defined in either live camera test or video recording test\n",
    "\n",
    "# Calculate the frame rate\n",
    "num_frames = len(filtered_frames)\n",
    "timelapse_length = 20\n",
    "frame_rate = int(num_frames / timelapse_length)\n",
    "\n",
    "# Define the codec and create a video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "out = cv2.VideoWriter(\"timelapse.mp4\", fourcc, frame_rate, (400, 500))\n",
    "\n",
    "# Define the transformation matrix\n",
    "for count, frame in enumerate(filtered_frames):\n",
    "    bbox = bbox_list[count][0]\n",
    "    keypoint = filtered_keypoints[count]\n",
    "\n",
    "    # pt1 = np.float32([keypoint[0],keypoint[3],keypoint[1],keypoint[2]])\n",
    "    pt1 = np.float32([keypoint[1],keypoint[3],keypoint[0],keypoint[2]])\n",
    "    pt2 = np.float32([[0,0],[400,0],[0,500],[400,500]])\n",
    "    M = cv2.getPerspectiveTransform(pt1,pt2)\n",
    "\n",
    "    # Perform the transformation\n",
    "    warped_image = cv2.warpPerspective(frame, M, (400, 500))\n",
    "    # warped_image = cv2.cvtColor(warped_image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    out.write(warped_image)\n",
    "\n",
    "# Release the video writer\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "645c3dacd5f5f6b81d66bfd9ee6b3c842761e9eb01ad3e7acabf5d8e8de65f87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
