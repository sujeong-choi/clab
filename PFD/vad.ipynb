{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice Activity Detector (VAD) in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hisku\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JIT file\n",
    "jit_model = torch.jit.load(\"./assets/vad_model/vad.jit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisku\\AppData\\Local\\Temp\\ipykernel_10460\\4115080836.py:17: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:205.)\n",
      "  data_tensor = torch.from_numpy(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.8024e-35,  3.2214e-36,  6.1718e-37,  ..., -5.5293e+35,\n",
      "         -3.9456e+35, -3.9456e+35]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/models/number_vad_model.py\", line 17, in forward\n    x0 = (_0)[0]\n    x1 = (self.adaptive_normalization).forward(x0, )\n    x2 = (self.encoder).forward(x1, )\n          ~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    _1 = self.sigmoid\n    _2 = torch.squeeze((self.decoder).forward(x2, ), -1)\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_20.py\", line 39, in forward\n    _14 = getattr(self, \"14\")\n    input0 = (_0).forward(input, )\n    input1 = (_1).forward(input0, )\n              ~~~~~~~~~~~ <--- HERE\n    input2 = (_2).forward(input1, )\n    input3 = (_3).forward(input2, )\n  File \"code/__torch__/models/transformer_modules.py\", line 23, in forward\n    else:\n      x0 = x\n    attn = (self.attention).forward(x0, )\n            ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    x2 = torch.add(x0, (self.dropout1).forward(attn, ), alpha=1)\n    x3 = (self.norm1).forward(x2, )\n  File \"code/__torch__/models/transformer_modules.py\", line 54, in forward\n    bsz, seq, dim, = torch.size(x)\n    head_dim = torch.floordiv(dim, self.n_heads)\n    _4 = torch.chunk((self.QKV).forward(x, ), 3, -1)\n                      ~~~~~~~~~~~~~~~~~ <--- HERE\n    q, k, v, = _4\n    _5 = torch.contiguous(torch.transpose(k, 0, 1), memory_format=0)\n  File \"code/__torch__/torch/nn/quantized/dynamic/modules/linear.py\", line 24, in forward\n      else:\n        _4 = self._packed_params._packed_params\n        Y2 = ops.quantized.linear_dynamic(x, _4, True)\n             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        Y0 = Y2\n      Y = Y0\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 119, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/quantized/dynamic/modules/linear.py\", line 47, in forward\n                    x, self._packed_params._packed_params)\n            else:\n                Y = torch.ops.quantized.linear_dynamic(\n                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n                    x, self._packed_params._packed_params, reduce_range=True)\n        elif self._packed_params.dtype == torch.float16:\nRuntimeError: In ChooseQuantizationParams, min should be less than or equal to max\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(data_tensor)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Send data to AI model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m output \u001b[39m=\u001b[39m jit_model(data_tensor)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(output)\n\u001b[0;32m     27\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hisku\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/models/number_vad_model.py\", line 17, in forward\n    x0 = (_0)[0]\n    x1 = (self.adaptive_normalization).forward(x0, )\n    x2 = (self.encoder).forward(x1, )\n          ~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    _1 = self.sigmoid\n    _2 = torch.squeeze((self.decoder).forward(x2, ), -1)\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_20.py\", line 39, in forward\n    _14 = getattr(self, \"14\")\n    input0 = (_0).forward(input, )\n    input1 = (_1).forward(input0, )\n              ~~~~~~~~~~~ <--- HERE\n    input2 = (_2).forward(input1, )\n    input3 = (_3).forward(input2, )\n  File \"code/__torch__/models/transformer_modules.py\", line 23, in forward\n    else:\n      x0 = x\n    attn = (self.attention).forward(x0, )\n            ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    x2 = torch.add(x0, (self.dropout1).forward(attn, ), alpha=1)\n    x3 = (self.norm1).forward(x2, )\n  File \"code/__torch__/models/transformer_modules.py\", line 54, in forward\n    bsz, seq, dim, = torch.size(x)\n    head_dim = torch.floordiv(dim, self.n_heads)\n    _4 = torch.chunk((self.QKV).forward(x, ), 3, -1)\n                      ~~~~~~~~~~~~~~~~~ <--- HERE\n    q, k, v, = _4\n    _5 = torch.contiguous(torch.transpose(k, 0, 1), memory_format=0)\n  File \"code/__torch__/torch/nn/quantized/dynamic/modules/linear.py\", line 24, in forward\n      else:\n        _4 = self._packed_params._packed_params\n        Y2 = ops.quantized.linear_dynamic(x, _4, True)\n             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        Y0 = Y2\n      Y = Y0\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 119, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/quantized/dynamic/modules/linear.py\", line 47, in forward\n                    x, self._packed_params._packed_params)\n            else:\n                Y = torch.ops.quantized.linear_dynamic(\n                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n                    x, self._packed_params._packed_params, reduce_range=True)\n        elif self._packed_params.dtype == torch.float16:\nRuntimeError: In ChooseQuantizationParams, min should be less than or equal to max\n"
     ]
    }
   ],
   "source": [
    "# use pyaudio to get audio stream\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=pyaudio.paInt16,\n",
    "                channels=1,\n",
    "                rate=16000,\n",
    "                input=True,\n",
    "                frames_per_buffer=8000)\n",
    "\n",
    "while True:\n",
    "    data = stream.read(8000)\n",
    "\n",
    "    # Convert audio data from bytes to numpy array\n",
    "    data = np.frombuffer(data, dtype=np.float32)\n",
    "\n",
    "    # Convert numpy array to a tensor\n",
    "    data_tensor = torch.from_numpy(data)\n",
    "    data_tensor = torch.reshape(data_tensor, (1, 4000))\n",
    "    data_tensor = torch.nan_to_num(data_tensor)\n",
    "    print(data_tensor)\n",
    "    \n",
    "    # Send data to AI model\n",
    "    output = jit_model(data_tensor)\n",
    "\n",
    "    print(output)\n",
    "\n",
    "    break\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\Hisku/.cache\\torch\\hub\\master.zip\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m\n\u001b[0;32m     21\u001b[0m stream \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mopen(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mpyaudio\u001b[39m.\u001b[39mpaInt16,\n\u001b[0;32m     22\u001b[0m                 channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     23\u001b[0m                 rate\u001b[39m=\u001b[39m\u001b[39m16000\u001b[39m,\n\u001b[0;32m     24\u001b[0m                 \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m                 frames_per_buffer\u001b[39m=\u001b[39mSAMPLING_RATE)\n\u001b[0;32m     28\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     data \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39;49mread(SAMPLING_RATE)\n\u001b[0;32m     30\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfrombuffer(data, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     31\u001b[0m     data_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(data)\n",
      "File \u001b[1;32mc:\\Users\\Hisku\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot input stream\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39;49mread_stream(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream, num_frames,\n\u001b[0;32m    571\u001b[0m                       exception_on_overflow)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use pyaudio to get audio stream\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "speech_probs = []\n",
    "window_size_samples = 512 # use 256 for 8000 Hz model\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " _, read_audio, VADIterator,\n",
    " *_) = utils\n",
    "\n",
    "vad_iterator = VADIterator(model)\n",
    "\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=pyaudio.paInt16,\n",
    "                channels=1,\n",
    "                rate=16000,\n",
    "                input=True,\n",
    "                frames_per_buffer=SAMPLING_RATE)\n",
    "\n",
    "\n",
    "while True:\n",
    "    data = stream.read(SAMPLING_RATE)\n",
    "    data = np.frombuffer(data, dtype=np.float32)\n",
    "    data_tensor = torch.from_numpy(data)\n",
    "\n",
    "    chunk = data_tensor\n",
    "    if len(chunk) < window_size_samples:\n",
    "      break\n",
    "    speech_dict = vad_iterator(chunk, return_seconds=True)\n",
    "    if speech_dict:\n",
    "        print(speech_dict, end=' ')\n",
    "\n",
    "    # speech_prob = model(chunk, SAMPLING_RATE).item()\n",
    "    # speech_probs.append(speech_prob)\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "vad_iterator.reset_states()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(speech_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d32610e65d0ba547d20b5ddccd11b31c7d91644470808fb362c82b58c120951"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
